{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Summarization Using Pegasus\n",
    "This notebook implements a document summarization pipeline using Google's Pegasus model. It extracts text from various document formats, processes it for summarization, and generates concise summaries using Pegasus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checks if Apple's Metal Performance Shaders (MPS) backend is available for running the model on Mac's GPU; otherwise, it defaults to CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import logging\n",
    "import PyPDF2\n",
    "import pytesseract\n",
    "import docx\n",
    "from bs4 import BeautifulSoup\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger('text_extractor')\n",
    "\n",
    "class TextExtractor:\n",
    "    \"\"\"A utility for extracting text from various document formats.\"\"\"\n",
    "    \n",
    "    SUPPORTED_FORMATS = {\n",
    "        \"pdf\": lambda path: extract_text_from_pdf(path).replace(\"\\n\", \" \"),\n",
    "        \"docx\": lambda path: extract_text_from_docx(path).replace(\"\\n\", \" \"),\n",
    "        \"doc\": lambda path: extract_text_from_doc(path).replace(\"\\n\", \" \"),\n",
    "        \"txt\": lambda path: extract_text_from_txt(path).replace(\"\\n\", \" \"),\n",
    "        \"html\": lambda path: extract_text_from_html(path).replace(\"\\n\", \" \"),\n",
    "    }\n",
    "    \n",
    "    @classmethod\n",
    "    def extract(cls, file_path: str, ocr_if_needed: bool = True) -> str:\n",
    "        if not os.path.exists(file_path):\n",
    "            raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "\n",
    "        _, ext = os.path.splitext(file_path)\n",
    "        ext = ext[1:].lower() if ext else \"\"\n",
    "        \n",
    "        if not ext:\n",
    "            raise ValueError(\"Cannot determine file type (no extension)\")\n",
    "            \n",
    "        if ext not in cls.SUPPORTED_FORMATS:\n",
    "            raise ValueError(\n",
    "                f\"Unsupported file format: {ext}. Supported formats: {', '.join(cls.SUPPORTED_FORMATS.keys())}\"\n",
    "            )\n",
    "        \n",
    "        try:\n",
    "            if ext == \"pdf\":\n",
    "                text = extract_text_from_pdf(file_path).replace(\"\\n\", \" \")\n",
    "                if not text.strip() and ocr_if_needed:\n",
    "                    logger.info(f\"No text found in PDF, attempting OCR: {file_path}\")\n",
    "                    text = extract_text_from_scanned_pdf(file_path).replace(\"\\n\", \" \")\n",
    "            else:\n",
    "                text = cls.SUPPORTED_FORMATS[ext](file_path)\n",
    "                \n",
    "            return text.strip()\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting text from {file_path}: {str(e)}\")\n",
    "            raise RuntimeError(f\"Failed to extract text: {str(e)}\")\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with open(pdf_path, \"rb\") as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            for page in reader.pages:\n",
    "                extracted_text = page.extract_text()\n",
    "                if extracted_text:\n",
    "                    text += extracted_text + \" \"\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error opening PDF file: {str(e)}\")\n",
    "        raise\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def extract_text_from_scanned_pdf(pdf_path: str, dpi: int = 300, lang: str = 'eng') -> str:\n",
    "    try:\n",
    "        images = convert_from_path(pdf_path, dpi=dpi)\n",
    "        text_parts = [pytesseract.image_to_string(img, lang=lang).strip() for img in images]\n",
    "        return \" \".join(text_parts)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during OCR processing: {str(e)}\")\n",
    "        raise RuntimeError(f\"OCR processing failed: {str(e)}\")\n",
    "\n",
    "def extract_text_from_docx(docx_path: str) -> str:\n",
    "    try:\n",
    "        doc = docx.Document(docx_path)\n",
    "        return \" \".join(para.text.strip() for para in doc.paragraphs if para.text.strip())\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error extracting DOCX content: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def extract_text_from_doc(doc_path: str) -> str:\n",
    "    try:\n",
    "        result = subprocess.run([\"antiword\", doc_path], capture_output=True, text=True, check=True)\n",
    "        return result.stdout.strip().replace(\"\\n\", \" \")\n",
    "    except FileNotFoundError:\n",
    "        raise RuntimeError(\"antiword is not installed. Install it to process DOC files.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        raise RuntimeError(f\"antiword failed with error code {e.returncode}: {e.stderr}\")\n",
    "\n",
    "def extract_text_from_txt(txt_path: str) -> str:\n",
    "    encodings = ['utf-8', 'latin-1', 'windows-1252']\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            with open(txt_path, \"r\", encoding=encoding) as file:\n",
    "                return file.read().strip().replace(\"\\n\", \" \")\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    raise RuntimeError(f\"Failed to decode file with supported encodings: {encodings}\")\n",
    "\n",
    "def extract_text_from_html(html_path: str) -> str:\n",
    "    try:\n",
    "        with open(html_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            soup = BeautifulSoup(file, \"html.parser\")\n",
    "            return soup.get_text(separator=\" \").strip()\n",
    "    except UnicodeDecodeError:\n",
    "        with open(html_path, \"r\", encoding=\"latin-1\") as file:\n",
    "            soup = BeautifulSoup(file, \"html.parser\")\n",
    "            return soup.get_text(separator=\" \").strip()\n",
    "\n",
    "def extract_text(file_path: str) -> str:\n",
    "    return TextExtractor.extract(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above function defines a text extraction utility that processes different document formats (PDF, DOCX, DOC, TXT, and HTML) to extract their textual content. It leverages various libraries such as PyPDF2 for PDFs, pytesseract for OCR on scanned PDFs, docx for Word documents, BeautifulSoup for HTML parsing, and subprocess for handling .doc files using antiword. The TextExtractor class manages supported formats through a dictionary mapping file extensions to extraction functions. The extract method determines the file type, validates its existence, and applies the appropriate extraction method, logging errors when necessary. If a PDF lacks extractable text, the script attempts OCR-based text extraction using pytesseract and pdf2image. The utility also includes robust error handling for missing files, unsupported formats, and encoding issues in text files. The script is designed for flexibility and scalability, making it useful for document processing applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/nelson/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from transformers import PegasusTokenizer\n",
    "import re\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "model_name = \"google/pegasus-large\"\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "def preprocess_for_pegasus(text, max_length=1024):\n",
    "    \"\"\"\n",
    "    Prepares text for Pegasus summarization.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Raw extracted text.\n",
    "        max_length (int): Maximum token length for Pegasus.\n",
    "        \n",
    "    Returns:\n",
    "        list: List of text chunks that fit within the token limit.\n",
    "    \"\"\"\n",
    "    # Remove HTML tags\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    # Sentence tokenization\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    for sentence in sentences:\n",
    "        # Get token count for the sentence without special tokens\n",
    "        tokenized_sentence = tokenizer.encode(sentence, add_special_tokens=False)\n",
    "        token_count = len(tokenized_sentence)\n",
    "        # If adding this sentence exceeds the limit, store the current chunk and reset\n",
    "        if current_length + token_count > max_length:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = [sentence]\n",
    "            current_length = token_count\n",
    "        else:\n",
    "            current_chunk.append(sentence)\n",
    "            current_length += token_count\n",
    "    # Append the final chunk if non-empty\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cleans and tokenizes text before passing it to the Pegasus model.\n",
    "- Splits long text into chunks (ensuring each fits within the token limit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "\n",
    "\n",
    "def summarize_text_with_pegasus(text, tokenizer, model, device,\n",
    "                                max_input_length=1024,\n",
    "                                max_output_length=256):\n",
    "    \"\"\"\n",
    "    Summarizes long text using Google Pegasus.\n",
    "\n",
    "    Args:\n",
    "        text (str): The full input text.\n",
    "        tokenizer: The Pegasus tokenizer.\n",
    "        model: The Pegasus model.\n",
    "        device: The device to run inference on (e.g., \"cuda\" or \"cpu\").\n",
    "        max_input_length (int): Maximum input token length.\n",
    "        max_output_length (int): Maximum output token length.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (generated summary (str))\n",
    "    \"\"\"\n",
    "    text_chunks = preprocess_for_pegasus(text, max_length=max_input_length)\n",
    "    summaries = []\n",
    "\n",
    "    for idx, chunk in enumerate(text_chunks):\n",
    "        if not chunk.strip():\n",
    "            print(f\"Skipping empty chunk {idx}.\")\n",
    "            continue\n",
    "\n",
    "        # Prepend the \"summarize:\" prompt if desired (based on how the model was trained)\n",
    "        input_text = \"summarize: \" + chunk.strip()\n",
    "\n",
    "        # Tokenize input text with truncation\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=max_input_length)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        if inputs[\"input_ids\"].shape[1] == 0:\n",
    "            print(\"⚠️ Skipping empty tokenized chunk.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Generate summary for the current chunk\n",
    "            summary_ids = model.generate(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                max_length=max_output_length,\n",
    "                num_beams=5,\n",
    "                early_stopping=True\n",
    "            )\n",
    "            summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "            summaries.append(summary)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error processing chunk {idx}: {e}\")\n",
    "            continue\n",
    "\n",
    "    full_summary = \" \".join(summaries) if summaries else \"No summary generated.\"\n",
    "    \n",
    "   \n",
    "    \n",
    "    return full_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Preprocessing the Input Text**\n",
    "- Calls preprocess_for_pegasus(text, max_length=1024) to split long text into smaller chunks.\n",
    "- Ensures that each chunk does not exceed the model’s token limit.\n",
    "- Stores the processed text chunks in text_chunks.\n",
    "2. **Iterating Through Text Chunks**\n",
    "- Loops over each chunk using enumerate(text_chunks).\n",
    "- Skips empty chunks with if not chunk.strip() to avoid processing unnecessary data.\n",
    "- Prepares input text by adding the \"summarize:\" prefix, as Pegasus may require prompts.\n",
    "3. **Tokenizing and Moving to Device**\n",
    "- Converts each chunk into tokenized form using tokenizer().\n",
    "- Uses PyTorch tensors (return_tensors=\"pt\") for model compatibility.\n",
    "- Moves the tokenized input to the specified device (CPU/GPU).\n",
    "4. **Generating the Summary**\n",
    "- Calls model.generate() to produce the summarized text.\n",
    "- Uses:\n",
    "\t- num_beams=5 for better quality results.\n",
    "\t- max_length=256 to limit the summary size.\n",
    "\t- early_stopping=True to prevent unnecessary generation.\n",
    "- Decodes and appends the generated summary.\n",
    "5. **Handling Errors and Returning Output**\n",
    "- Handles exceptions to prevent crashes when processing large texts.\n",
    "- Concatenates all summary chunks into a single final summary.\n",
    "- Returns the summarized text or \"No summary generated.\" if errors occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 11:06:20,864 - datasets - INFO - PyTorch version 2.6.0 available.\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "BERT Scores:-  Precision: 0.8837 Recall: 0.9849 F1_score: 0.9316\n",
      "Compression Ratio: 5.10\n",
      "Readability Score: 22.75\n",
      "Generated Summary: Seeking a position to apply my comprehensive drink knowledge and service skills to enhance the bar program and guest experience in a professional establishment.Profile Work Experience Bartender | Forge (Bank) | 26/07/2023 – present Prepared and presented a wide array of cocktails (30+), mocktails, and speciality drinks with precision.\n"
     ]
    }
   ],
   "source": [
    "from textstat import flesch_reading_ease\n",
    "import evaluate\n",
    "\n",
    "# Load Pegasus model & tokenizer \n",
    "model_name = \"google/pegasus-large\"\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "ref  = \" Seeking a position to apply my comprehensive drink knowledge and service skills to enhance the bar program and guest experience in a professional establishment.Profile Work Experience Bartender\"\n",
    "raw_text = extract_text(\"Bartender.pdf\")\n",
    "\n",
    "# Generate summary\n",
    "summary = summarize_text_with_pegasus(\n",
    "    text=raw_text,\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    device=device,\n",
    "    max_input_length=1024,\n",
    "    max_output_length=256\n",
    ")\n",
    "\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "bertscore_results = bertscore.compute(\n",
    "    predictions=[summary],  # Must be a list\n",
    "    references=[ref],       # Must be a list\n",
    "    lang=\"en\"\n",
    ")\n",
    "\n",
    "# Compute compression ratio\n",
    "original_length = len(raw_text.split())\n",
    "summary_length = len(summary.split())\n",
    "compression_ratio = original_length / summary_length if summary_length > 0 else 0\n",
    "\n",
    "# Compute readability score\n",
    "readability = flesch_reading_ease(summary)\n",
    "\n",
    "# Print results\n",
    "print(\"Evaluation Metrics:\")\n",
    "print(f\"BERT Scores:-  Precision: {bertscore_results['precision'][0]:.4f} Recall: {bertscore_results['recall'][0]:.4f} F1_score: {bertscore_results['f1'][0]:.4f}\")\n",
    "print(f\"Compression Ratio: {compression_ratio:.2f}\")\n",
    "print(f\"Readability Score: {readability:.2f}\")\n",
    "\n",
    "print(\"Generated Summary:\", summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Loads Pegasus Model**\n",
    "– Imports necessary libraries and loads \"google/pegasus-large\" for text summarization.\n",
    "\n",
    "2. **Extracts Text** \n",
    "– Reads text from \"Bartender.pdf\" using extract_text().\n",
    "\n",
    "3. **Generates Summary** \n",
    "– Uses summarize_text_with_pegasus() to summarize the extracted text.\n",
    "\n",
    "4. **Evaluates Summary** \n",
    "– Computes BERTScore (Precision, Recall, F1-score) by comparing the summary with a reference text.\n",
    "\n",
    "5. **Computes Metrics** \n",
    "– Calculates compression ratio (original vs. summary length) and readability score, then prints all results. 🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
