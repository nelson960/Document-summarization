{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Analysis of the Fine-Tuning Process in This Notebook**\n",
    "This notebook fine-tunes Google's Pegasus model on the CNN/DailyMail dataset for text summarization. The fine-tuning process follows a structured workflow, including data loading, preprocessing, tokenization, training, and evaluation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code checks whether Apple's Metal Performance Shader (MPS) is available (for Mac M1/M2/M3). If not, it defaults to CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['document', 'summary', 'id'],\n",
      "    num_rows: 287113\n",
      "})\n",
      "{'document': '(CNN)Share, and your gift will be multiplied. That may sound like an esoteric adage, but when Zully Broussard selflessly decided to give one of her kidneys to a stranger, her generosity paired up with big data. It resulted in six patients receiving transplants. That surprised and wowed her. \"I thought I was going to help this one person who I don\\'t know, but the fact that so many people can have a life extension, that\\'s pretty big,\" Broussard told CNN affiliate KGO. She may feel guided in her generosity by a higher power. \"Thanks for all the support and prayers,\" a comment on a Facebook page in her name read. \"I know this entire journey is much bigger than all of us. I also know I\\'m just the messenger.\" CNN cannot verify the authenticity of the page. But the power that multiplied Broussard\\'s gift was data processing of genetic profiles from donor-recipient pairs. It works on a simple swapping principle but takes it to a much higher level, according to California Pacific Medical Center in San Francisco. So high, that it is taking five surgeons, a covey of physician assistants, nurses and anesthesiologists, and more than 40 support staff to perform surgeries on 12 people. They are extracting six kidneys from donors and implanting them into six recipients. \"The ages of the donors and recipients range from 26 to 70 and include three parent and child pairs, one sibling pair and one brother and sister-in-law pair,\" the medical center said in a statement. The chain of surgeries is to be wrapped up Friday. In late March, the medical center is planning to hold a reception for all 12 patients. Here\\'s how the super swap works, according to California Pacific Medical Center. Say, your brother needs a kidney to save his life, or at least get off of dialysis, and you\\'re willing to give him one of yours. But then it turns out that your kidney is not a match for him, and it\\'s certain his body would reject it. Your brother can then get on a years-long waiting list for a kidney coming from an organ donor who died. Maybe that will work out -- or not, and time could run out for him. Alternatively, you and your brother could look for another recipient-living donor couple like yourselves -- say, two more siblings, where the donor\\'s kidney isn\\'t suited for his sister, the recipient. But maybe your kidney is a match for his sister, and his kidney is a match for your brother. So, you\\'d do a swap. That\\'s called a paired donation. It\\'s a bit of a surgical square dance, where four people cross over partners temporarily and everybody goes home smiling. But instead of a square dance, Broussard\\'s generous move set off a chain reaction, like dominoes falling. Her kidney, which was removed Thursday, went to a recipient, who was paired with a donor. That donor\\'s kidney went to the next recipient, who was also paired with a donor, and so on. On Friday, the last donor will give a kidney to someone who has been biding time on one of those deceased donor lists to complete the chain. Such long-chain transplanting is rare. It\\'s been done before, California Pacific Medical Center said in a statement, but matching up the people in the chain has been laborious and taken a long time. That changed when a computer programmer named David Jacobs received a kidney transplant. He had been waiting on a deceased donor list, when a live donor came along -- someone nice enough to give away a kidney to a stranger. Jacobs paid it forward with his programming skills, creating MatchGrid, a program that genetically matches up donor pairs or chains quickly. \"When we did a five-way swap a few years ago, which was one of the largest, it took about three to four months. We did this in about three weeks,\" Jacobs said. But this chain wouldn\\'t have worked so quickly without Broussard\\'s generosity -- or may not have worked at all. \"The significance of the altruistic donor is that it opens up possibilities for pairing compatible donors and recipients,\" said Dr. Steven Katznelson. \"Where there had been only three or four options, with the inclusion of the altruistic donor, we had 140 options to consider for matching donors and recipients.\" And that\\'s divine, Broussard\\'s friend Shirley Williams wrote in a comment her on Broussard\\'s Facebook page. \"You are a true angel my friend.\"', 'summary': 'Zully Broussard decided to give a kidney to a stranger .\\nA new computer program helped her donation spur transplants for six kidney patients .', 'id': 'a4942dd663020ca54575471657a0af38d82897d6'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "samples_fraction = 0.1\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "dataset = dataset.rename_columns({\"article\": \"document\", \"highlights\": \"summary\"})\n",
    "train_data = dataset[\"train\"].shuffle(seed=42).select(range(int(len(dataset[\"train\"]) * samples_fraction)))\n",
    "val_data = dataset[\"validation\"].shuffle(seed=42).select(range(int(len(dataset[\"validation\"]) * samples_fraction)))\n",
    "\n",
    "print(dataset[\"train\"])\n",
    "print(dataset[\"validation\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Loads the CNN/DailyMail dataset, a benchmark dataset for abstractive text summarization.\n",
    "- The dataset has:\n",
    "- \"article\" (the news article) → Renamed as \"document\".\n",
    "- \"highlights\" (the human-written summary) → Renamed as \"summary\".\n",
    "- To speed up training, only 10% (samples_fraction = 0.1) of the dataset is used for fine-tuning.\n",
    "- Training and validation data are shuffled and randomly selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function preprocess_function at 0x317062e80> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f92d2ae3f834b9f88224812935127b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/28711 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "156f5abc0dee4d0d9a68d3cc7290b8f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1336 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 28711\n",
      "}) Dataset({\n",
      "    features: ['id', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 1336\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "\n",
    "\n",
    "# tokenizer\n",
    "model_name = \"google/pegasus-large\"\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# preprocessing \n",
    "def preprocess_function(examples):\n",
    "    inputs = tokenizer(\n",
    "        examples[\"document\"], \n",
    "        padding=\"max_length\", \n",
    "        truncation=True, \n",
    "        max_length=1024\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        examples[\"summary\"], \n",
    "        padding=\"max_length\", \n",
    "        truncation=True, \n",
    "        max_length=256\n",
    "    )\n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "tokenized_train = train_data.map(preprocess_function, batched=True, remove_columns=[\"document\", \"summary\"])\n",
    "tokenized_val = val_data.map(preprocess_function, batched=True, remove_columns=[\"document\", \"summary\"])\n",
    "\n",
    "print(tokenized_train, tokenized_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Pegasus tokenizer is loaded using transformers.PegasusTokenizer.\n",
    "- The preprocessing function:\n",
    "\t1. Tokenizes the \"document\" (full article) and truncates it to 1024 tokens.\n",
    "\t2. Tokenizes the \"summary\" and truncates it to 256 tokens.\n",
    "\t3. Stores the tokenized summary under \"labels\", making it suitable for supervised learning.\n",
    "- Tokenized datasets (tokenized_train and tokenized_val) replace \"document\" and \"summary\" columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PegasusForConditionalGeneration, Trainer, TrainingArguments, BitsAndBytesConfig\n",
    "\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(device) \n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "\n",
    "# Define training arguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./pegasus_finetuned\",\n",
    "    per_device_train_batch_size=1,  \n",
    "    gradient_accumulation_steps=4,  \n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=3,\n",
    "    bf16=True, \n",
    "    save_total_limit=2,  \n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###-  Model Setup\n",
    "- Loads Pegasus-large, a transformer-based sequence-to-sequence model optimized for abstractive text summarization.\n",
    "- Moves the model to Apple MPS (or CPU).\n",
    "- Enables gradient checkpointing, which reduces memory usage by recomputing activations during backpropagation.\n",
    "\n",
    "### Training Arguments\n",
    "- Batch Size: 1 (small batch size to avoid memory overload).\n",
    "- Gradient Accumulation: 4 steps (accumulates gradients over multiple mini-batches to simulate larger batches).\n",
    "- Evaluation: Performed at the end of each epoch.\n",
    "- Learning Rate: 5e-5, a standard fine-tuning rate for transformers.\n",
    "- Epochs: 3 (fine-tunes for three passes over the training data).\n",
    "- Mixed Precision: Uses bfloat16 (bf16=True), which improves efficiency on Apple M1/M2/M3.\n",
    "- Checkpointing: Limits saved models to the latest two (save_total_limit=2).\n",
    "- Logging: Saves logs every 100 steps.\n",
    "\n",
    "### How Fine-Tuning Works\n",
    "- Uses the Hugging Face Trainer API, which simplifies training and evaluation.\n",
    "- The loss function used is cross-entropy loss, as this is a sequence-to-sequence task.\n",
    "- During fine-tuning:\n",
    "- The document (input) is passed through the Pegasus encoder.\n",
    "- The decoder generates token sequences for the summary.\n",
    "- Loss is computed between the generated summary and the ground truth summary.\n",
    "- Backpropagation updates the model’s weights to minimize the loss.\n",
    "- The model is evaluated on the validation set after each epoch.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
